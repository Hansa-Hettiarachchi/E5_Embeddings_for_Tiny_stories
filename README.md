# E5_Embeddings_for_Tiny_stories

Tiny Stories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-41. The dataset was introduced in a paper titled "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?" by Ronen Eldan and Yuanzhi Li [1]. The authors show that Tiny Stories can be used to train and evaluate language models that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. The dataset is available on the Hugging Face website.Â 

## Creating Embeddings using E5 model for Tiny Stories dataset
E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs,
E5, which stands for "EmbEddings from bidirEctional Encoder rEpresentations," is an innovative approach to training embeddings. To get to know more about E5 model refer <a href = "https://medium.com/@hansahettiarachchi/unleashing-the-potential-of-embedding-model-e5-revolutionizing-natural-language-comprehension-3f1516489048">[this]</a>.
